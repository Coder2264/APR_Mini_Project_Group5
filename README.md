# APR_Mini_Project_Group5
This repository includes mini project for Advanced Pattern Recognition prepared by members of Group 5

# Signature Forgery Detection: An Academic Overview

This document outlines the theoretical approach and machine learning pipeline we have implemented in the Mini project for identifying forged signatures.

---

## 1. Problem Statement

The objective is to develop a computational model capable of classifying a given signature image as either **"Real"** (genuine) or **"Forged"** (fraudulent). This is a binary image classification problem, a common task in pattern recognition and biometric security. The model must learn to differentiate the subtle, often high-frequency, spatial features that distinguish a person's authentic signature from a skilled imitation.

---

## 2. Methodology

The approach is a classical machine learning pipeline that transforms raw image data into a low-dimensional feature space. This is achieved by chaining three distinct stages:

1.  **Feature Extraction**: The high-dimensional pixel data of each preprocessed image (e.g., grayscaled, resized, blurred) is converted into a meaningful numerical representation. This project uses the **Histogram of Oriented Gradients (HOG)** for this purpose.
2.  **Dimensionality Reduction**: The feature vector generated by HOG is very large (20,736 features in this case). To reduce computational complexity, prevent overfitting, and remove redundant information, **Principal Component Analysis (PCA)** is applied.
3.  **Classification**: A robust classification algorithm, the **Support Vector Machine (SVM)**, is trained on the reduced-dimensional feature vectors to learn a decision boundary that best separates the "Real" and "Forged" classes.



---

## 3. Theoretical Concepts and Algorithms

### Histogram of Oriented Gradients (HOG)

**Theory**: HOG is a feature descriptor used primarily for object detection in computer vision. Its core idea is that the local shape and appearance of an object (like a signature) can be accurately described by the distribution of intensity gradients or edge directions.

**Procedure**:
1.  **Gradient Calculation**: Compute the gradient (magnitude and orientation) for each pixel in the image.
2.  **Cell Histograms**: Divide the image into small spatial regions called "cells." For each cell, a histogram of gradient orientations is compiled.
3.  **Block Normalization**: Group adjacent cells into larger "blocks." These blocks are used to normalize the histograms within them, providing robustness to changes in illumination and contrast.
4.  **Feature Vector**: The histograms from all normalized blocks are concatenated to form the final, comprehensive feature vector for the entire image.

In this notebook, HOG effectively captures the stroke, curvature, and contour information of the signature, which are critical features for distinguishing authenticity.

### Principal Component Analysis (PCA)

**Theory**: PCA is a statistical procedure used for unsupervised dimensionality reduction. It uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of linearly uncorrelated variables called **principal components**.

**Procedure**:
1.  **Covariance Matrix**: PCA computes the covariance matrix of the high-dimensional feature set (the HOG vectors).
2.  **Eigendecomposition**: It finds the eigenvalues and eigenvectors of this matrix. The eigenvectors represent the "directions" of maximum variance in the data, and the eigenvalues represent the "magnitude" of that variance.
3.  **Component Selection**: The eigenvectors are sorted by their corresponding eigenvalues in descending order. The first 'k' eigenvectors (where 'k' is the desired number of new dimensions) are chosen. These 'k' components are the principal components that capture the most variance from the original data.
4.  **Projection**: The original data is projected onto this new 'k'-dimensional subspace.

In this project, PCA reduces the 20,736 HOG features down to just **200 principal components**, drastically simplifying the data while preserving the most significant information for classification.

### Support Vector Machine (SVM)

**Theory**: SVM is a powerful supervised learning algorithm used for classification and regression. For classification, its objective is to find the optimal hyperplane that maximizes the margin (the distance) between the data points of different classes.

**Procedure**:
* **Hyperplane**: In a 2D space, this is a line; in a 3D space, it's a plane. In the 200-dimensional space of this project, it is a 199-dimensional hyperplane.
* **Support Vectors**: These are the data points closest to the hyperplane, which are the most critical for defining its position and orientation.
* **Kernel Trick**: SVMs can efficiently perform non-linear classification using the kernel trick. By projecting the data into a higher-dimensional space, a linear hyperplane can be found to separate data that was non-linearly separable in the original space.
* **Polynomial Kernel**: This notebook employs a **polynomial kernel (`kernel='poly'`)**. This is effective for problems where the decision boundary is not a simple straight line, allowing the model to capture more complex relationships between the signature features.

---

## 4. Experimental Procedure (Pipeline)

The end-to-end procedure for classifying a new signature image follows these steps:

1.  **Input**: An image of a signature is provided.
2.  **Preprocessing**: The image is converted to grayscale, resized to a uniform 200x200 pixels, and a 3x3 Gaussian blur is applied to reduce noise.
3.  **HOG Feature Extraction**: The preprocessed image is passed through the HOG descriptor to generate a 20,736-feature vector.
4.  **PCA Transformation**: The HOG vector is transformed by the pre-fitted PCA model, reducing it to a 200-component vector.
5.  **SVM Prediction**: This 200-component vector is fed into the trained SVM (with a polynomial kernel).
6.  **Output**: The SVM model outputs a final classification: **0 (Real)** or **1 (Forged)**.

---

## 5. Conclusion

This project successfully implements a signature forgery detection system by combining HOG for robust feature extraction, PCA for efficient dimensionality reduction, and an SVM with a polynomial kernel for high-accuracy classification. The final model demonstrates a high capability (94.44% accuracy on the test set) in distinguishing between genuine and forged signatures based on their learned structural features.
